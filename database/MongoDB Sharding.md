# Sharding

여러 장비(Shard)에 데이터를 여러 조각(Chunk)으로 나누어 분산하여 저장하는 과정

서버를 여러개 두어 분산 저장하여 I/O가 여러 대에서 일어나기에 효율이 좋아짐.

- Scale Out
- horizontal sacle



### 용도

- 여러 장치를 두어 많은 데이터를 저장하고 많은 부하 처리 가능
- **읽기/쓰기**
    - 읽기 및 쓰기를 분산시켜 처리
    - 자주 접근하는 데이터를 성능 좋은 하드웨어(샤드)에 배치하여 빠른 접근 가능
- **HA (고가용성)**
    - config 서버 및 샤드를 복제본 세트로 배포하여 가용성이 향상


💡 **Replica vs Shard**

- Replica : 여러 서버에 데이터 복사본 즉, 모두 동일
- Shard : 각 샤드에 각기 다른 데이터 존재


## 구성요소



### Config Server (구성서버)

- sharded cluster 두뇌부로 다음의 정보를 저장합니다.
    - metadata 정보담은 collection
        - 상태값
        - 클러스터 설정 및 구성요소
        - chunk list
    - 클러스터 내부 인증관련 정보
- mongodb 3.4 이상 부터 replica set 형태로 구성가능
    - 데이터 복구를 위함
    - replica set 구성에 제한 존재
        - arbiter
        - index 필요
        - delayed member


💡 구성 서버 데이터를 자주 백업하고, 클러스터 유지 보수 수행 전 구성 서버 백업 필요.



### mongos

> 클라이언트가 접근하는 인터페이스
>
- config server로 부터 metadata를 가져와 캐싱한 후 query를 적절한 샤드로 routing 하는 역할을 합니다.
    - 전달의 목적을 가진 라우터 서버로 데이터를 찾는 일은 수행하지 않습니다.
- 적은 수의 mongos 프로세스가 필요
    - HA(고가용성)을 위해선 최소 2개 필요
    - 수많은 mongos 실행가능하지만 네트워킹, CPU 리소스 측면 경합 유발
- 가능한 모든 샤드에 가깝게 배치

### Shard

- 샤딩을 통해 생성된 것으로 분산된 데이터를 포함하는 저장공간
- 최소한 2개 이상의 Shard 서버가 존재해야 Sharding의 목적에 적절합니다.
- replica set으로 배포 권고
    - 데이터 복구를 위함.
- **mongos를 통해서만** query를 전달 받습니다.
    - 직접 query 전달이 가능하지만 데이터 손실의 가능성 존재

**Primary Shard**

- 샤딩되지 않은 collection을 Primary Shard에 포함
- Primary Shard 와 Replica set 상의 primary는 연관이 없습니다.
- `movePrimary` 명령어 통해서 primary shard 변경 가능

  

- 정리 이미지

  

  


## 환경별 구성

### Production



### Development



## Shard Key



Mongo DB는 샤드 키를 사용해 데이터를 특정 샤드가 소유한 청크로 분할합니다.


💡 **청크**

- 샤드 내의 샤드 키 값 범위.
- 청크 범위에는 하한 경계는 포함되고 상한 경계는 제외됩니다. 예를 들어 3, 17 사이의 모든 도큐먼트를 하나의 청크로 보면 3≤ x <17 를 의미합니다.
- 기본 청크 크기는 128메가바이트입니다. 1~1024MB의 크기가 허용되며 이를 수정할 수 있습니다.
    - Shard에 더 많은 I/O 이용하는 서비스일 경우 크기를 줄이는 것이 좋습니다.
- MongoDB는 청크가 구성된 청크 크기보다 커지면 청크를 분할합니다.
  예를 들어, 3≤ x <17의 청크에 해당하는 도큐먼트들이 너무 많아지면 3≤x<12, 12≤x<17로 분할될 수 있습니다.
- MongoDB는 샤드에 다른 샤드에 비해 컬렉션의 청크가 너무 많을 때 청크를 마이그레이션합니다. (청크 분할)
- 데이터 크기 감소하면 일부 청크가 필요하지 않기에 최적화를 위해 **병합**을 진행하기도 합니다.


- 샤드키를 선정해야 데이터 분할이 가능합니다.
    - 샤드키는 두 가지의 형태로 구성될 수 있습니다.
        - 단일 필드를 사용한 샤드키
        - 여러 필드를 조합한 복합 샤드키
    - 배열 field는 샤드키로 사용하지 못합니다.
        - 성능저하, 인덱스 제한 등의 다양한 이유
- 샤딩 이후에는 샤드 키 변경이 불가능합니다.
    - mongoDB 5.0 이상부터는 컬렉션의 샤드 키 변경히여 reshard 가능
- 기존 샤드키에 접미사를 하나이상 추가해 샤드키 구체화 가능
    - `refineCollectionShardKey`

        ```python
        db.adminCommand( {
           refineCollectionShardKey: "test.orders",
           key: { customer_id: 1, order_id: 1 }
        } )
        ```


### Shard key 선정시 고려사항

Shard key 선정은 Document를 균등하게 배포하고 쿼리 패턴을 갖추는데에 큰 영향을 가집니다. 대부분 Sharding의 수평적 확장이라는 이점을 해하는 부분들입니다.

1. Shard Key Cardinality
    - Shard Key의 카디널리티 → 최대 청크의 수를 결정합니다. 그렇기에 가능하다면 카디널리티가 높은 샤드 키를 선택해야합니다.
    - 카디널리티가 낮은 샤드 키는 Sharding의 수평적 확장 효율성에 해를 가합니다. 해당 경우에는 복합 인덱스를 통해 샤드 키의 카디널리티를 높입니다.

   

2. 샤드 키 빈도
    - 데이터 발생 빈도로 해당 값이 포함된 Document를 저장하는 chunk에서 병목현상이 발생 가능합니다.
    - 너무 빈도가 높아 data의 덩어리가 커지면 분할 불가능(**점보 청크**)합니다. 결국 Sharding의 특징인 수평적 확장의 효율성에 해를 가합니다.
    - 데이터가 높은 빈도의 값을 갖는 키에 대한 샤딩의 경우 복합 인덱스를 사용하기도 합니다.

   

3. monotonic한게 변화하는 Shard Key
    - 단조롭게 증감 하는 샤드 키의 경우 단일 청크(**최대청크**)에만 데이터가 쌓일 수 있습니다. 이로 인한 병목 현상이 발생할 수 있습니다.

   

4. Sharding Query Pattern
    - Shard key 선택 시 서비스의 쿼리 패턴 또한 고려합니다. 자주 쿼리되는 필드로 선택합니다. 그렇지 않으면 Broadcast query 발생하여 효율성이 떨어집니다.
5. Shard Key Limitation
    - 다음의 인덱스는 사용이 불가능합니다.
        - 내림차순 인덱스
        - partial index


💡 In mongoDB 7.0
`analyzeShardKey` 를 통해 지표를 계산할 수 있습니다.

- keyCharacteristics : cardinality, freqeuncy, monotonic 에 대한 지표
- readWriteDistribution : query routing 패턴, 부하 분산에 대한 지표


### Shard Key 통한 샤딩 과정

MongoDB에서 데이터 분산 방법을 알려주기 전에는 데이터를 분산하지 않습니다. 분산하려는 디비 및 컬렉션을 명시적으로 알려주고 shard key를 지정해야합니다.

책에 있는 예시를 사용했습니다.
music db의 artists 컬렉션에서 “name” 키로 샤딩합니다.

- `sh.enableSharding(databasem primaryShard)`
    - version 6.0 이상부터는 필요없습니다.
- `sh.shardCollection`(”music.artists”, {”name” : 1} )
    - name 필드가 인덱스가 존재하지 않으면 오류를 반환합니다.
- 성공 시 mongoDB에서 샤드에 컬렉션을 분산하며 이는 시간이 오래 걸릴 수 있습니다.
    - 사전 분할을 통해 시간을 줄일 수 있습니다. 이렇게 하면 추가적인 밸런싱 작업 없이 샤드에 삽입됩니다.


💡 **사전 분할**
가능한 샤드 키 값의 범위를 청크로 분할하여 삽입이 용이하고 쓰기 처리량이 높습니다.



## Sharding 전략

### Range Sharding

샤드 키 값의 범위에 따라 데이터를 샤드에 분배하는 방식으로 기본 샤딩 방식

주로 데이터의 논리적인 범위 또는 시간적인 범위에 따라 분할됩니다.



**특징**

- 다음의 Shard Key인 경우 효율적입니다.
    - non monotonic하게 바뀌는 shard key
    - 카디널리티가 높은 Shard key
    - low frequency 의 shard key
- 데이터의 위치가 명시적
    - 가까운 shard key값을 가진 문서는 동일한 chunk에 존재할 가능성이 큽니다.
- 범위 내의 대상 문서를읽는 경우 효율적인 쿼리가 가능합니다. 즉, 검색 속도의 빠름.
    - target query
- 데이터 불균형 초래되어 트래픽 불균형 발생
    - 내주적으로 migration 또한 생기기에 부하 발생
- 샤드 추가 혹은 삭제의 어려움

### Hashed Sharding

샤드 키의 해시 값에 따라 데이터를 분배하는 방식

다음의 명령어로 해시 인덱스 지정 후 샤드 키로 이용해야합니다.

- `sh.shardCollection( "database.collection", { <field> : "hashed" } )`

  


**특징**

- 다음의 Shard Key의 경우 효율적입니다.
    - 카디널리티가 높은 Shard key
    - monotonic 한 Shard Key
- 불균등한 데이터를 클러스터 전체에 효율적으로 분산시켜 균등하게 분산 시킵니다.
- 범위 조건의 Query를 수행하는 경우에는 적절하지 않습니다.
    - 해당 경우에는 broadcast query이 수행되어 성능적인 이슈를 유발할 수 있습니다.
- 데이터 위치가 랜덤

### Zone

Shard Cluster에서 Shard Key를 통해 영역을 생성 후 각 영역을 클러스터에 있는 하나 이상의 샤드와 연결할 수 있습니다.



**특징**

- 샤드는 여러 영역과 연결될 수 있습니다.
- 다음의 경우에 자주 사용합니다.
    - zone을 사용해 데이터를 위치별로 분산 시키기 위함 (서버, 데이터 센터)
    - 위치별 데이터 세분화
    - 고객별, 애플리케이션 별 데이터 세분화를 통한 특정 샤드로의 데이터 격리
    - 하드웨어/성능 기반 데이터 라우팅
- range + hash

  

  

- latency를 낮춰서 빠른 응답을 받을 수 있습니다.

## 밸런서

- 샤드의 데이터 양을 모니터링하는 백그라운드 프로세스
- 주기적으로 샤드를 체크합니다. 특정 샤드에서 임계점에 도달 시 샤드 간에 데이터를 자동으로 마이그레이션을 진행합니다.
    - 동시 마이그레이션의 수는 샤드당 하나
    - 병렬 데이터 마이그레이션 수행 가능 하지만 최대 동시 마이그레이션 수는 총 샤드 수의 절반
    - **임계값 기준** : 샤드 간의 데이터 차이가 3배 이상인 경우 마이그레이션 수행 (MongoDB 6.0부터)
        - 기본 크기가 128MB인 경우 두 샤드의 데이터 크기 차이가 384MB 이상이여야 수행
- 밸런서 프로세스는 항상 활성화되어 있습니다.
    - 유지 관리를 위해 밸런서를 일시적으로 비활성화하기도 가능합니다.
- 밸런싱 과정은 클러스터 이용하는 애플리케이션이 이에 대해 정확히 알지 못해도 문제 없습니다.
- 밸런싱 과정에서 성능적인 이슈 발생할 수 있습니다.
- zone 지키면서 이동



### **밸런싱 과정**

- 한 샤드 임계치 도달
- 밸런서가 데이터 마이그레이션 진행
    - `moveRange` 를 source 샤드로 보냅니다.
    - 샤드가 명령 수신시 마이그레이션 시작
    - target 샤드에 존재하지 않는 데이터에 필요한 인덱스 구축 후 복사 시작
    - 마무리 후 동기화 프로세스를 통해 변경 사항의 유무 확인
- config서버의 metadata 갱신


💡 1. 밸런싱 과정에서 밸런서가 데이터를 이동시키는 과정에서 들어오는 읽기/쓰기 요청에 대해서는 이동 전의 청크로 전달됩니다.

2. 메타데이터 갱신 후 이전 위치의 데이터에 접근 시도하는 mongs 프로세스는 오류를 발생하지만 mongos가 은밀히 오류를 처리하고 새 샤드에 작업을 시도하기에 클라이언트는 에러발생을 모릅니다. 이것이 밸런싱 과정을 애플리케이션이 몰라도 되는 이유 중 하나입니다.



### **밸런싱 정책 변경**

Mongo DB가 6.1이 되고 나서의 변경 사항

- 밸런서는 청크가 아닌 데이터 범위를 분산. 즉, 청크 분산 말고 데이터 분산의 균등성을 목표로 함.
    - 정밀한 쿼리 최적화
    - 유연한 스케일링
    - 밸런싱 및 재배치의 효율
    - 세분화된 데이터 분산
- 청크는 자동 분할되지 않습니다.
- `movwRange` → `moveChunk`

### 밸런서 사용없이 수동 마이그레이션 하는 경우

- 빈 collection을 사전 분할 하는 경우
    - 관련 공식문서 자료 : https://www.mongodb.com/ko-kr/docs/manual/tutorial/create-chunks-in-sharded-cluster/
- active cluster의 밸런서가 기간 안에 분산하지 못하는 경우

## Replica set vs Sharded Cluster

### 선택 기준

1) 서비스의 요구사항 확인

2) 배포 환경 확인

3) DB 배포

위 단계에서 두 배포 방식의 장단점을 비교하며 결정해야합니다.

### 장단점 비교

**Replica set**

- 장점
    - 운영이 쉽다
    - 장애 발생 시 문제 해결 및 복구가 쉽다.
    - 서버 비용이 상대적으로 적게 든다.
    - 성능이 좋다.
    - 개발 설계에 용이하다.
- 단점
    - Read에 대한 분산이 가능하지만 Write에 대한 분산은 불가능하다.

**Sharded Cluster**

- 장점
    - Scale-out 이 가능
    - Write에 대한 분산이 가능하다.
- 단점
    - Replica set의 모든 장점이 상대적 단점으로 작용한다.
    - 내결함성이 떨어집니다. → replica set구성으로 해결
        - 성능?

### 예시 상황

1. 매일 1GB씩 데이터가 증가하고 3년관 보관 → 경우에 따라 다름

   

2. write 요청이 압도적으로 많은 서비스 → Sharded Cluster
    - 증권 어플
3. 논리적인 데이터 베이스가 많은 경우 → 여러 Replicas set 으로 분리



### 성능 비교

- 샤드를 2개만 사용하여 hash shard와 range shard로 쿼리를 보낸 것과 replica set에서 쿼리를 보낸 것





### 결론

- replica set 으로검토
- 서비스 요구사항에서 충족이 되지 않으면 Sharded Cluster이용