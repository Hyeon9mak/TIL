### replication 간단 요약
MongoDB 에서 Replica set 은 완전히 동일한 데이터 셋을 유지하는 process group 을 의미한다. 중복성을 내어준 대신 고가용성을 얻을 수 있다. (당연하게도) 단일 DB 일 때의 단일 장애포인트 문제를 해결해주기도 한다.

### Replication in MongoDB
- MongoDB 에서는 replica set 을 생성함으로서 replication 을 설정할 수 있다.
	- 클라이언트의 요청을 처리하는 primary server 1 대
	- primary data 의 복사본을 갖는 secondary server N 대
- primary server 가 죽으면, secondary server 중 1 대가 primary sever 로 선출된다.
- secondary server 가 죽으면, 유기적으로 다른 secondary server 로 접근이 가능하다.
- 기본적으로 클라리언트는 secondary server 를 직접 지목하여 읽기 작업을 수행할 순 없다. 'secondary 에서 읽고 있음을 알고 있다.' 를 뜻하는 설정을 연결에 명시적으로 설정하면 그제부터 읽기를 활성화 할 수 있다. `secondaryConn.setSlaveOk()`

### replica set

> 과반수(majority): replica set 내 모든 멤버의 절반보다 많은 것

![](https://i.imgur.com/Kg0VvPY.png)

primary 를 선출하려면 멤버의 과반수 이상이 필요하다. primary 는 과반수 이상이어야만 primary 자격을 유지할 수 있고, 쓰기 연산은 과반수 이상에 복제되면 안전하다고 판단한다.

예를 들어 replica set 의 멤버가 5개이고, 그 중 3개가 다운되었다고 가정하자. 여전히 2개의 멤버는 살아있다.
2개의 멤버는 replica set 의 과반수가 되지 않으므로, primary 를 선출할 수 없다. 만약 primary 였다면 primary 자격을 바로 포기한다. 즉 2개의 secondary 와 3개의 다운 된 멤버로 구성된다.

![](https://i.imgur.com/9tCXt8i.png)

왜 2개의 멤버에서는 primary 를 선출하지 않을까?
다른 3개의 멤버가 자체적으로 다운 된 것이 아니라 단순히 네트워크 오류가 발생한 것일 수도 있기 때문이다. 왼쪽의 3개 멤버는 과반수에 해당되므로(5 of 3) 자체적으로 primary 를 선출할 수도 있다. 즉 하나의 replica set 에 두 개의 primary 가 생성될 수 있고, 이렇게 되면 2개의 쓰기 연산이 진행되면서 데이터 셋이 나뉘는 위험이 생긴다.

이처럼 primary 를 하나만 갖도록 replica set 을 구성하는게 매우 중요하다.
멤버 1,2,3 이 하나의 DC(data center)에 있고, 나머지가 다른 DC 에 있다면 복제셋 과반수에 의해 primary 는 언제나 멤버 1,2,3 이 속한 DC 에 있을 가능성이 높다.

primary 가 둘 이상이 될 경우 쓰기 충돌관리를 지원해야한다. 즉, 개발이 어렵고 복잡해진다.
MongoDB 는 오직 단일 primary 만 지원한다. replica set 이 읽기 전용 상태일 때는 어느정도 시간이 걸리지만, 그 결과 개발이 쉬워진다.

## 멤버 구성 옵션
특정 멤버가 우선적으로 primary 가 되게 하거나, 클라이언트에 보이지 않도록 해 읽기 요청이 도달하지 않도록 할 수 있다. 이를 가능케 하는 아래 옵션들이 존재한다.

### 우선순위(priority)
- 0~100 값 지정이 가능하며 기본은 1이다. 
- 숫자가 클수록 primary 로 선정되는 우선순위가 높아진다.
- 0으로 지정하면 그 값은 절대 primary 가 될 수 있다.

### 숨겨진 멤버(hidden member)
- 요청을 절대 전ㄷ라하지 않는다.
- 다만 숨겨진 멤버는 replica source 로 적당하지 않다.
- 주로 백업 서버를 숨겨놓는 용도

### 아비터
- 소규모 프로젝트에서 3개 이상의 멤버를 다루는 것은 과하게 느껴질 수 있다.
- 3개 내부터 관리, 운영, 비용을 고려하면 큰 가치가 없다.
- 이러한 배포에 대해 primary 선출에 참여하는 용도로만 쓰이는 아비터라는 특수한 멤버가 존재.
- 데이터를 가지지 않으며, 클라이언트의 요청이 도달되지 않는다.
- 오로지 2멤버 replica set 에서 과반수를 만들기 위해서만 사용된다.
	- 노드의 개수가 홀수이면 아비터는 필요하지 않다.
- 일반적으로는 아비터가 없는게 바람직하다.

주의점도 당연히 존재한다.
- 아비터는 최대 1개까지만
	- 여분의 아비터가 생긴다고 선출 속도가 빨라지지 않는다.
	- 추가 안정성을 제공해줄 수도 없다.
- 아비터와 데이터 노드를 골라야한다면 차라리 데이터 노드를 선택하라. 아니면 운영 업무가 어려워진다.
	- 2멤버 상황에서 아비터를 도입하면 1멤버가 죽을시 데이터 복제를 해야만 한다.
	- 반대로 3멤버를 만들면 새로운 primary 가 선출된다. 차라리 이게 낫다.

### 인덱스 관리
- 간혹 secondary 가 primary 에 존재하는 것과 동일한 인덱스를 갖지 않아도 될 것이다. 한 마디로 인덱스가 필요 없는 상황이 있다.
	- secondary 를 데이터 백업이나 오프라인 배치 작업에만 사용하는 경우
- 이럴 땐 `"buildIndexes" : false` 를 멤버 구성에 명시하면 된다.
	- 당연히 멤버의 priority 는 0 으로 관리해야한다.
- 이는 영구적인 설정이며 `"buildIndexes" : false` 가 명시된 멤버는 인덱스를 구출할 수 있는 일반 멤버로 재구성될 수 없다.
	- 다시 바꾸려면 replica set 에서 멤버를 제거하고 데이터를 모두 지운 뒤 다시 동기화 해야한다.

## 동기화
### replica set 멤버가 새로운 데이터를 복제하는 법
- 몽고DB 는 primary 가 수행한 쓰기를 모두 포함하는 로그(`oplog`)를 보관함으로써 복제를 수행한다. 
- `oplog` 는 primary 의 local db 에 있는 컬렉션
- 각 secondary 는 primary 로부터 복제한 작업을 기록하는 `oplog`를 각각 갖는다.
- secondary 는 이 컬렉션에 복제를 위한 연산 쿼리를 던진다.
	- 만약 연산 적용에 실패하면 secondary 는 종료된다.
	- 실패 1: 기본 데이터에 오류 발생
	- 실패 2: primary 랑 oplog 가 달라짐
- secondary 가 어떤 이유로든 다운되면, 재시작시 `oplog` 에 있는 마지막 연산과 동기화한다.
	- `oplog` 는 여러번 재생해도 한번 재생할 때와 같은 결과를 얻는다. 멱등성 보장.
- `oplog` 는 크기가 정해져있다. 일반적으로 기본 크기면 충분함
	- 별개 옵션으로 지정해줄 수 있음. 아래와 같은 경우 늘리자. `oplogSizeMB`
	- 한 번에 여러 document 를 갱신
	- 삽입한 데이터와 동일한 양의 데이터 삭제
	- 상당한 수의 내부 갱신

#### 복제
- secondary 멤버는 초기 동기화 후 지속적으로 데이터를 복제한다.
- 동기화 소스에서 oplog 를 복사한 후, 비동기로 복제를 진행한다.
- secondary 동기화 중 수행된 실제 연산보다 훨신 뒤떨어지면 실효상태에 도입한다.
	- 동기화 대상의 모든 연산이 secondary 보다 훨씬 앞서기 ㄸ문에 모든 연산을 따라잡기 힘들어진다.
	- 이 경우 각 멤버로부터 차례로 복제를 시도해서 긴 oplog 를 가진 멤버가 있는지 확인한다.
	- 충분히 긴 oplog를 갖는 멤버를 발견하지 못하면 해당 멤버에서 복제가 중지되고 완전히 재동기화 되어야 한다.

#### 멤버들의 상태와 하트비트
- 멤버는 다른 멤버의 상태 정보, 즉 누가 프라이머리이고, 누구로부터 동기화하며, 누가 다운됐는지 등을 알아야 한다.
- 멤버는 복제 셋에 대한 최신 정보를 유지하기 위해 복제 셋의 모든 멤버로 2초마다 하트비트 요청을 보낸다.
	- 하트비트 요청은 모두의 상태를 점검하는 짧은 메시지다.
- 하트비트의 가장 중요한 기능은 복제 셋의 과반수 도달 가능 여부를 primary 에게 알리는 기능이다.
- 프라이머리가 더는 서버의 과반수에 도달할 수 없다면 스스로를 강등해 세컨더리가 된다.

#### 멤버들의 상태
- STARTUP
	- 멤버 처음 시작 상태. 구성 정보 로드를 시도할 때.
	- 구성 정보가 로드되면 STARTUP2 가 된다.
- STARTUP2
	- 초기 동기화 과정 전반에 걸쳐 지속.
	- replication 과 선출을 위해 스레드가 분기되며, RECOVERING 으로 변환
- RECOVERING
	- 멤버가 올바르게 동작중이지만, 아직 읽기 작업은 수행할 수 는 상태. 조금 과부하 된 상태에서 나타난다.
	- 또는 멤버가 너무 많이 뒤쳐졌을 때 따라잡기 위해 도입
	- 오류로 판단하진 않는다. 누군가 긴 `oplog` 를 갖고 있어서 금방 회복될 수 있기 때문
- ARBITER
	- 아까 본 특수상태
- DOWN
	- 살아는 있지만 요청이 전달되지 않는 상태
- UNKNOWN
	- 멤버가 다른 멤버에 도달한 적이 단 한번도 없는 경우
	- 보통 네트워크간 문제가 있음을 나타냄
- REMOVED
	- 멤버가 replica set 으로부터 제거된 상태
	- 제거된 멤버가 replicat set 에 다시 추가되면 정상적인 상태가 된다.
- ROLLBACK
	-  데이터를 롤백할 때 사용. 롤백 마지막 과정에서 RECOVERING 으로 전환되고 secondary 가 된다.

### 선출 동작 원리
- 멤버가 primary 를 찾지 못하는 상태에서 자신이 primary 가 될 자격이 있다면, primary 선출을 모색한다.
- 선출되고자 하는 멤버는 전달할 수 있는 모든 멤버에게 알림을 보낸다.
- 알림을 받는 멤버들은 해당 멤버가 primary 가 될 자격이 있는지 알아본다.
	- 복제가 뒤쳐지거나, 이미 primary 가 존재하거나 등을 확인한다.
	- 반대할 이유가 있으면 반대 투표를 진행한다.
- 반ㄷ개가 딱히 없으면 선출되고자 하는 멤버에 투표를 진행한다.
	- 해당 멤버가 복제셋의 과반수로 득표하면 선출된다. primary 가 된다.
	- 과반수로부터 득표하지 못하면 멤버는 secondary 상태로 남으며 나중에 다시 시도한다.
- 한번 primary 가 되면 아래와 같은 상황 전까지 자격을 유지한다.
	- 멤버의 과반수에 도달할 수 없거나
	- 다운 되거나
	- 세컨더리로 강등 되거나
	- 복제셋이 재구성 되거나

선출은 대부분의 상황에서 빠르게 진행된다. primary 가 다운되었다고 알리는데 최대 초 소요 후 선출을 즉시 시작하여 몇 ms 내에 종료된다. 하지만 네트워크에 문제가 거나 너무 느리게 응답하는 서버 때문에 선출이 발생할 수도 있다. 이 때 하트비트는 타임아웃 때문에 최대 몇 분 단위로 시간을 소요할 수도 있다.

### 롤백
![](https://i.imgur.com/RIS8E5Z.png)

DC2 가 과반수가 되어 primary 선출을 진행할 수 있다. 반대로 DC1 은 primary 를 잃는다.

네트워크가 복구되면 DC1 멤버들은 동기화를 시도하려하지만, 연산 #126 을 찾지 못한다.
이런 현상이 발생하면 A 와 B 는 `롤백` 을 시작한다.

- 롤백은 복구 전 복제되지 않은 연산을 원래 상태로 되돌리는데 사용한다.
- 다른 DC 멤버들의 `oplog` 를 살핀다.
- 가장 최신연산(#125)를 발견한 후 겹치는 지점의 작업들을 rollback 디렉터리에 있는 `.bson` 파일에 작성한다.

![](https://i.imgur.com/30AILIz.png)

- 그 후 현재 primary 에서 해당 도큐먼트의 버전을 복제한다.

## 애플리케이션에서 replica set 연결
### client-replica set connect
- 몽고 드라이버는 서버가 독립 실행형 몽고 인스턴스건, 복제 셋이건 관계없이 통신을 관리할 수 있다.
- replica set 이면 기본적으로 드라이버는 primary 에 연결되고, 모든 트래픽을 primary 로 routing 한다.
- app 은 replica set 이 조용히 백그라운드에서 대기 상태를 유지하는 동안 마치 독립형 서버와 통신하듯 write/read 작업을 수행할 수 있다.

- 몽고 드라이버에서 MongoClient 를 사용하고, 연결할 드라이버를 위한 시드 목록을 제공한다.
	- 시드는 단순한 replica set 의 멤버 주소 목록
	- 애플리케이션이 데이터를 읽고 쓸 replica set 의 멤버다.
	- 모든 멤버를 나열하진 않는다. 할 필요가 없으므로. 할 순 있음.
	- 시드에 연결되면 다른 멤버들을 발견할 수 있다.

- 서비스를 운영하다보면 아주 짧은 시간동안 primary 가 존재하지 않을 수 있다.
	- 기본적으로 드라이버는 이 기간동안 연산(read/write)을 처리하지 않는다.
	- 만약 read 요청이 필요하다면 그 순간에 secondary 를 사용해 read 는 가능하도록 드라이버를 구성할 수 있다.
- 드라이버는 보통 재선출 과정을 사용자에게 숨기고 싶어하지만, 그러지 못한다.
	- 왜? 드라이버는 primary 의 부재만 숨길 수 있다.
	- 드라이버는 primary 가 다운 됨을 연산 실패로 인해 알게 될 때가 많다.
		- primary 가 down 되기 전 연산 수행 여부를 드라이버는 알지 못한다는 뜻
		- 분산 시스템의 근본적인 문제로, 방어할 방법이 없다.
		- 따라서 적절한 처리 전략이 필요.
	- 새 primary 에서 작업을 다시 시도해야할까?
	- 이전 primary 에서 수행되었다고 가정할까?
	- 새로운 primary 가 해당 연산을 갖는지 확인할까? <-- 이 대목에서 `oplog 는 멱등성을 보장한다를 떠올려보자`.
- 올바른 전략은 최대 1회만 재시도 하는 것
	- 재시도 하지 않기
		- 일시적인 네트워크 순단 발생이라면 연산 횟수가 부족해진다.
	- 일정 횟수만큼 재시도한 후 포기하기
		- 일시적인 네트워크 순단이라면 연산 횟수가 너무 많아진다.
	- 최대 1번만 재시도 하기
		- 어차피 멱등성을 보장하므로 연산을 1회 더해도 괜찮다.
		- 너무 과하지 않은 적당한 해결 방법

### write 직후 replication 대기
- application 의 요구에 따라 모든 멤버에 write 동작이 수행되기 전까지 잠시 대기하도록 요구할 수 있다.
- 드물게 primary 가 중단되고 새로운 primary 가 마지막 write 를 이전 primary 가 복제해주지 않으면, 이전 primary 가 다시 primary 가 될 때 쓰기가 롤백된다.
	- 수동개입이 필요한 부분.
	- 대부분 애플리케이션에서는 큰 문제가 아니긴함. 댓글 하나가 날라간다던지...

> -   그러나 신뢰도가 높은 서비스에서는 쓰기 롤백은 가능하면 회피하자.
> - 애초에 그런 서비스에서 mongo DB 를 쓰는게 맞을지 고민이 필요해보인다.

- 결국 이러한 복잡한 상황을 방지하기 위해선 쓰기 연산이 replica set 멤버 과반수 이상에 전파돼야한다.
	- 이 때 `writeConcern` 을 사용한다.
	- MongoDB 2.6 부터 쓰기 작업과 통합
- 멤버는 쓰기 작업이 replica set 과반수 이상에 복제될 때까지 응답하지 않는다.
	- 복제가 되고 나서야 애플리케이션이 쓰기가 성공했다는 응답을 받는다.
	- 제한시간 내 쓰기가 성공하지 못하면 timeout 에러가 발생한다.
	- 당연히 과반수에 숨겨진 멤버(hidden member)는 제외횐다.

![](https://i.imgur.com/PetK7H7.png)

- 쓰기 확인 결과 과반수 대상 멤버 선정과 replica set 선출 프로토콜은 승인된 쓰기가 있는 최신 secondary 만 primary 로 선출되도록 제어한다.
- 이 방식 덕분에 롤백이 발생하지 않는 것을 보장한다.
- 복제 셋의 과반수 이상에 쓰기가 완료되면 '안전'하다고 여겨진다.
	- 그러나 당연히 요구사항마다 상황이 다르다.
	- 요구사항마다 다른 '안전' 상황을 만들기 위해 `getLastError` 값에 사용자 규칙을 만들어 넘겨줄 수 있다.


### secondary 로 읽기 전송
#### 일관성 고려시
- 기본적으로 드라이버는 모든 요청을 primary 로 라우팅한다.
- 드라이버에서 읽기 선호도(read preference)를 설정해 다른 옵션을 구성할 수도 있다.
	- read 요청을 secondary 로 보내면 일반적으로 좋지 않다.
	- 일반적으로 모든 트래픽은 primary 로 전송해야한다.
	- 왤까?
- 매우 일관된 읽기가 필요한 애플리케이션이라면 secondary 로 부터 읽기를 수행하면 안된다.
	- secondary 가 보통 primary 와 몇 ms 내 오차가 좁혀져야한다.
	- 그러나 이를 보장하긴 매우 어렵다.
- 읽기 요청을 항상 primary 로 보내려면 read preferecne 를 primary 로 설정하자.

#### 부하 고려시
- 사실 많은 사용자들이 부하 분산을 위해 read 를 secondary 로 전송한다.
- 그러나 이러한 확장법은 위험하다.
	- 뜻하지 않게 시스템에 부하를 유발할 수 있고, 회복이 어렵다.
- 예를 들어 초당 30,000 읽기가 발생한다.
	- 읽기 처리를 위한 멤버가 4개의 개인 replica set 을 생성한다.
	- 각 secondary 의 부하는 최대 부하보다 적으며, 시스템은 완벽히 동작한다.
	- 이 중 secondary 하나가 손상될 경우 나머지 멤버들이 부하를 100% 처리한다.
	- 손상된 멤버를 재구축하려면 하나를 복사해야하는데, 100% 처리중인 멤버들을 곤란하게 만든다.
	- 과부하는 복제를 느리게 만들고, 복제는 100% 부하를 해결해야한다. 악순환.
- 부하 분산은 샤딩을 통해 관리하자. replication read 트래픽 분할로 관리하는 것이 아니다.

- 다만 몇몇 애플리케잉션은 secondary 로 read 를 수행하는게 합리적이다.
	- 예를 들어 primary 가 죽어도 지속적으로 read 작업을 수행해야할 때.
	- 이 읽기 선호도를 `primary Preferred` 라고 한다.
- 드라이버에서 replica set 멤버까지 평균 핑 시간을 기반으로 지연률이 낮은 멤버에 요청 라우팅
	- 그러나 이것도 결국 지연율이 낮은 read/write 를 원할 경우 샤딩이 필요해진다.

- 아직 모든 쓰기를 복제하지 못한 멤버로부터 read 를 수행하려면 당연히 일관성을 희생해야한다.
	- 반대로 속도를 희생한다면 일관성을 챙길 수 있다.
- `secondary` 또는 `secondaryPreperred` 선호도를 사용하면 항상 secondary 멤버에 읽기 요청을 라우팅한다. 
	- `secondary` 는 이용 가능한 secondary 가 없으면 primary 로 보내지 않고 에러를 발생시켜버린다.
	- `secondaryPreperred` 는 이용 가능한 secondary 가 없으면 primary 로 보낸다.