# 9장. S3와 유사한 객체 저장소

AWS S3(Simple Storage Service)와 유사한 객체 저장소 서비스 설계 과정.

<br>

## 대표적인 저장소 시스템들

저장소 시스템(storage system)을 크게 보면 3가지 부류가 있다.

1. 블록(block) 저장소
2. 파일(file) 저장소
3. 객체(object) 저장소

<br>

### 1. 블록 저장소

> HDD, SSD 처럼 서버 애플리케이션에 물리적, 직접적으로 연결되는 형태의 드라이브.
> 블록 저장소는 원시 블록(raw block)을 서버에 볼륨(volume) 형태로 제공한다.
> 가장 유연하고 융통성이 높은 저장소다.

여기서 raw block 은 OS FS 에서 주로 이야기하는, 4KB 단위를 가진 FS 의 최소 단위인 block 개념을 의미하는듯 하다.
아래 리눅스 운영체제 fs 에서 사용하는 block 단위를 인용하면 이해가 쉽겠다.

![image](https://github.com/user-attachments/assets/b0aae1c7-0241-40ac-a41f-6dc5764a30ea)

volume 은 raw block 들을 하나로 묶어 제공하는 단위. 즉, block 들을 다루기 위한 논리적인 개념이 담긴 시스템.
(block 들을 FS 로 다루면 더 상위 개념인 file 이 된다.)

유연하고 융통성이 높다는 것은? 원시적인 단위인 block 을 제공해주기 때문에 이들을 포맷시켜 FS 로 이용해버리거나,
서버 애플리케이션에 블록 단위로 제어권을 넘겨버릴 수도 있기 때문에 자유롭다고 표현했다.
대표적으로 DBMS, VM 등이 원시 블록을 직접 제어하기 때문에 OS 영역의 User buffer 를 생략하여, 최선의 성능을 뽑아내는 느낌.
(직접 block 의 주소까지 제어하는 등...)

> 블록 저장소는 저장단위가 중요한 개념일 뿐이므로, 꼭 물리적으로 연결 될 필요는 없다.
> 네트워크를 통해 연결될 수도 있고, FC(Fibre Channel), iSCSI 를 통해 연결될 수도 있다.

FC 는 TCP/IP보다 구조가 단순해 높은 안정성과 높은 트래픽 처리 속도를 갖고 있다고 한다.
가장 큰 특징은 파이버 채널의 데이터 전송 단위인 '프레임'을 여러 개 엮어 '시퀀스'로 전송하고,
프레임 처리가 하드웨어 레벨에서 이루어져 CPU의 오버헤드가 적다.

iSCSI(Internet Small Computer Systems Interface) 는 FC 의 비용, 거리, 호환성 문제를 해결하기 위해 고안된 네트워크 기술.

간단하게 말하면 그냥 block 단위를 사용하는 제공하는, 자유도 높은 저장소라는 것.

### 2. 파일 저장소

> 파일 저장소는 블록 저장소 위에 구현된다. 데이터는 계층적으로 구성되는 디렉터리 안에 보관된다.
> 파일 저장소를 사용하는 서버는 블록 제어, 볼륨 포맷 등을 신경쓸 필요가 없다.

모든 작업을 FS(File System)가 다 해주기 때문. 우리가 윈도우, 리눅스 등의 OS 를 다루며 매일 마주하던 그것이다.

### 3. 객체 저장소

> 데이터 영속성을 높이고 대규모 애플리케이션을 지원하며 비용을 낮추기 위해 의도적으로 성능을 희생하는 저장소.
> 상대적으로 '차가운(cold)' 데이터 보관에 초점을 맞추며, 데이터 아카이브나 백업에 주로 쓰인다.

'차가운' 이라는 표현은 데이터의 현재 형상만을 다루는 것이 아닌, 로그성으로 계속해서 데이터를 적재해 나가기 때문.
가령 "학생" 데이터의 "이름" 이 "최현구" 에서 "최현팔" 로 변경된다면, 객체 저장소에는 2개의 row 가 저장되어 있을 것이다.

```
학생 ID: 1, 이름: 최현구, 2024-09-25 22:44:12 
학생 ID: 1, 이름: 최현팔, 2024-09-25 23:31:09 
```

데이터의 영속성을 높인다는 표현도 이러한 특성 때문에 모든 변화가 추적 가능하고, 
데이터가 중간에 유실되더라도 어느정도 복구가 가능하기 때문.

> 모든 데이터를 수평적 구조 내에 객체로 보관하며, 계층적 디렉토리 구조는 제공하지 않는다.

성능이 구린 이유. 수평적으로 적재하기 때문에 계층을 이용한 접근이 불가능하여 성능적 이점을 누리기 어렵다.
객체 저장소 시스템 사용자에겐 계층적 디렉토리를 제공해줄 수 있다. `/` 과 같은 구분자를 기준으로
논리적인 계층 표현이 가능하기 때문이다.

```
계층을 나타내는 것 같지만, 다른 시야로 바라보면 단순한 문자열일 뿐이다.
ABC/DEF/GGG.png
ABC/DEF/HHH.jpg
```

### 객체 저장소 용어 정리

버킷(bucket): 객체를 보관하는 논리적인 컨테이너. 물리적으로는 하나, 혹은 여러개로 이루어질 수 있으나
사용자에게는 그것이 드러나지 않는다. OS/FS 에 비유하자면 볼륨, 파티션 개념.

객체(object): 버킷에 저장되는 각각의 데이터 덩어리. 하나의 객체는 data(혹은 payload) 와 metadata 로 구성된다.
RDS 로 비유하자면 metadata 들만 따로 모아 인덱스처럼 활용하면서 data 탐색의 효율을 높여볼 수 있다.

버전(versioning): 실수로 지웠거나 덮어 쓴 객체를 복구할 수 있도록 도와주는 그것.

URI(Uniform Resource Identifier): RESTful API 를 통해 접근하는데 사용되는 객체들의 식별자.

SLA(Service-Level Agreement): 서비스 수준 협약. SLA 를 잘 살펴보고 어떤 객체 저장소를 사용할지 결정하자.
아마존 S3 는 아래와 같은 SLA 를 만족한다고 공개한다.

- 여러 AZ 에 걸쳐 99.999999999% 의 객체 내구성 제공
- 하나의 AZ 전체가 소실되어도 복원 가능
- 연간 99.9% 의 가용성 제공

<br>

## 객체 저장소 규모 계산하기

- 연간 100PB 데이터 추가
- 99.9999% 수준의 데이터 내구성
- 99.99% 수준의 가용성
- 높은 수준의 안정성과 성능은 보장하되, 저장소 비용은 최대한 낮춰야한다.

100PB 규모의 데이터는 몇 개의 객체로 분류되어 저장될까?
다시 말하자면 어느정도 수준의 객체 저장이 가능한 저장소를 고민해야할까?
아래와 같은 가정을 세우고 계산을 해보자.

- 객체 중 20% 는 0.5MB
- 객체 중 60% 는 32MB
- 객체 중 20% 는 200MB

연간 40% 저장 공간 사용률을 유지하고자 하는 경우 수용 가능한 객체의 수는 아래와 같이 계산한다.

```
(연간 적재량 * 유지하고 싶은 사용률)
/
(비율 당 객체 용량)
```

```
100 PB = 100 * 10^9 MB = 10^11 MB

(10^11 * 0.4)
/
(0.2 * 0.5) + (0.6 * 32) + (0.2 * 200)
= 6억 8천만개.
```

실제 객체의 data(payload) 만 저정도이고, metadata 는 추가로 계산을 해야한다.
모든 객체의 metadata 크기가 1KB 라고 가정하면, `6.8억 * 1KB = 0.68TB` 정도의 공간이 추가로 필요하게 된다.

<br>

## 객체 저장소의 흥미로운 속성

객체 불변성(object immutability): 차가운(cold) 데이터 보관을 보장하는 특성.
객체 저장소에 보관되는 객체들은 모두 변경이 불가능하다. 
완전히 삭제 시킨 후 다시 새로운 버전으로 객체를 대체할 순 있어도, 기존에 존재하는 데이터를 변경할 순 없다.
계속해서 새로이 추가된다.

키-값 저장소(key-value store): URI 가 key 가 되고, 실제 data 가 value 가 된다.

거의 모든 요청이 조회: LinkedIn 에서 조사한 결과에 따르면 객체 저장소의 요청 중 95% 가 읽기 요청이라고 한다.
즉, 조회 성능에 초점을 두고 저장소를 구성해아한다.

소형 및 대형 객체 동시 지원: 다양한 크기의 객체를 문제없이 다룬다.

객체 저장소의 골짜는 결국 UNIX FS 와 아주 비슷하다.
UNIX FS 에서 파일을 하나 저장하면, 파일의 이름(metadata)과 데이터(data/payload)는 각기 다른 곳에 저장된다.
데이터(data/payload)는 디스크 어딘가 공간에 적재되고, 파일의 이름(metadata)는 inode 라고 불리는 자료구조에 적재한다.

![image](https://github.com/user-attachments/assets/3732f985-aeb8-4a45-b1cf-d8c06073b9ef)

그리고 inode 의 파일 block 포인터를 타고타고 이동해서 실제 데이터(data/payload)를 찾아낸다.

![image](https://github.com/user-attachments/assets/b53c76c6-c37a-4e93-9717-a6816a92b598)

객체 저장소도 동일한 결을 가졌다. 객체 저장소의 metadata 저장소는 inode 에 해당된다.
metadata 에는 파일 block 포인터 대신 ID 가 보관된다.

<img width="776" alt="image" src="https://github.com/user-attachments/assets/90b64d71-9f59-46fe-85b0-d16059a36afb">

사용자가 인지하는 bucket(논리 저장소)에는 실제로 metadata 만 존재하는 것이며, 실제 Data 는 다른 곳에 적재되어 있는 것.

<img width="702" alt="image" src="https://github.com/user-attachments/assets/4512ba10-f4fd-4980-b842-8bb6437433f5">

metadata 는 가변으로 관리하는 것이 용이하고, data 는 불변으로 관리하는 것이 용이하다.
각각을 독립적으로 저장하여 관리하기 때문에 서로 다른 구현 방식으로 최적화할 수 있다.

<img width="760" alt="image" src="https://github.com/user-attachments/assets/99524b37-0c5b-438b-95ab-4d41b15085e5">

그림을 살펴보면 data 저장소를 replication 하여 관리하는 것을 볼 수 있다.
사용패턴상 조회요청이 95% 나 되기 때문에 replication 효율이 굉장히 뛰어나다.

또한 metadata 만 모아서 관리하기 떄문에, 실제 데이터들은 분산관리(샤딩) 하기에도 용이하다.
MongoDB 와 같은 관리 구조를 가지고 있다!

### 객체 업로드 / 다운로드

<img width="647" alt="image" src="https://github.com/user-attachments/assets/286056ae-7280-420c-972a-3e980dad42cd">

data 저장소에서는 객체의 이름을 다루지 않는다. 객체의 ID(UUID)만 다룬다. 객체의 이름을 통해 data 에 접근하고자 할 땐 metadata 를 거쳐야한다.

<img width="689" alt="image" src="https://github.com/user-attachments/assets/3d8faca5-e236-4bb1-816f-ba77100041d5">

객체를 다운로드 하려면 객체 이름을 우선 ID 로 변환해야한다. 당연히 metadata 저장소를 거친다!

<br>

## 상세 설계 - 데이터 저장소

아래 그림은 data 저장소를 구성할 때 고려할 수 있는 컴포넌트들이다.

<img width="662" alt="image" src="https://github.com/user-attachments/assets/bedb1a6b-b186-45a2-a106-fb2cec2fbcfa">

데이터 라우팅 서비스는 규모확장을 쉽게 할 수 있도록 돕는 무상태 서비스다. RESTful, gRPC 서비스를 제공해줄 수 있다.
데이터를 적재하고, 데이터를 읽어 반환하는 일을 수행한다.

그럼 데이터 라우팅 서비스는 어떤 노드에 데이터를 적재할지 어떻게 결정할까?
사실 데이터 라우팅 자체도 데이터 트래픽을 감당하기에 바빠서, 배치 서비스에게 책임을 전가한다.
배치 서비스는 내부에 가상 클러스터 지도(VCM)를 갖고 있는데, 이를 통해 데이터를 적재하기 가장 적합한 데이터 노드를 빠르게 판단할 수 있다.

<img width="781" alt="image" src="https://github.com/user-attachments/assets/1b27677a-a147-4f3c-bd51-11ffedbfdc8e">

그림과 같은 예시에서는 트리구조를 갖고 있으므로, DC-host-parition 으로 이루어지는 주소체계를 잘 만들면 선택 최적화를 하기 좋을 것이다.
배치 서비스는 모든 데이터 노드 헬스를 체킹하는 역할도 수행한다. 일정시간동안 답변이 없으면 죽은 노드로 판단해버린다.
로드밸런서처럼, 배치 서비스는 그 자체로도 굉장히 바쁘기 때문에 적합한 데이터 노드 판단 역할만 수행한다.

> 배치 서비스는 5~7개의 노드를 갖는 배치 서비스 클러스터를 팩서스나 래프트 같은 합의 프로토콜을 사용하여 구축할 것을 권장한다.

팩서스 프로토콜과 래프트 프로토콜은 아래 링크에 기가 막히게 설명이 되어있다.
https://gruuuuu.github.io/integration/paxos-raft/

데이터노드는 실제 데이터가 보관되는 곳이다. replicaiton 을 통해 안정성과 내구성을 보증한다.
각 데이터 노드에는 배치 서비스에 하트비트 메세지를 보내는 서비스 데몬이 돈다. 메세지에는 주로 아래와 같은 내용이 담긴다.

- 디스크(HDD/SDD) 정보
- 저장된 데이터의 양

배치 서비스는 처음보는 데이터 노드에서 하트비트 메세지를 받으면 새로운 식별자를 부여하고, VCM 에 추가한 다음 아래 정보를 노드에게 준다.

- 식별자 (니 ID 야~)
- VCM (전체 구조는 이래~)
- 데이터 사본을 보관할 위치 (니가 replication 해줘야 할 주소야~)

<img width="679" alt="image" src="https://github.com/user-attachments/assets/7d266e4e-b638-4fd3-a25f-03988c50fa99">

### 데이터를 저장해보자!

1. API 서비스는 객체를 데이터 저장소로 포워딩한다.
2. 데이터 라우팅 서비스가 객체에 UUID 를 배급하고, 배치 서비스에게 물어봐서 어디에 적재할지를 결정한다.
   1. 배치 서비스는 VCM 을 참고한다.
3. 데이터 라우팅 서비스는 UUID 와 함께 데이터를 데이터 노드로 보낸다.
4. 데이터 노드는 자기 위치에 저장 후 replica 들에게 도 데이터를 보내어 다중화한다. replica 까지 저장을 완수하면 성공 응답을 보낸다.
5. API 에 객체 ID 를 반환한다.

배치 서비스는 주어진 ID(UUID) 를 기준으로 어떤 데이터노드에 데이터를 저장할지 어떻게 결정할까?
보통 안정해시를 사용한다.
안정해시는 해시링(hash ring)을 베이스로 요청 또는 데이터를 각 노드에 균등하게 나눌 수 있도록 도와주는 알고리즘이다.

아래 그림을 통해 간단한 예시를 들자면...

1. K0, K1 등 값을 통해 S0, S1 노드에 접근할 수 있다.
2. 링 구조를 갖고 있기 때문에 수평적 규모를 확장하여도 최소한의 KEY 만 재배치하면 된다. 새로 추가된 노드 인근의 KEY 만 재배치 될 뿐, 다른 KEY 들은 재배치 되지 않는다.
3. 제거도 마찬가지.
4. 가상 노드를 분산 배치시켜서 특정 노드가 너무 많은 키를 보관하는 문제를 해결한다.

<img width="657" alt="image" src="https://github.com/user-attachments/assets/177b6830-8c08-4124-9ba4-a52c36ac26c9">

자세한 내용은 이전 책의 5장 - 안정 해시 설계 를 참고하자.

객체 저장소의 데이터 노드가 응답을 반환하기 위해서는 모든 replica 의 성공을 기다려야하므로 속도가 발목을 붙잡는다.
그래서 3개 선택지를 통해 안정성과 속도 사이에 트레이드오프를 결정한다.

<img width="653" alt="image" src="https://github.com/user-attachments/assets/04aa35ec-ffa9-4685-ac51-045ac47f00ed">

2, 3번 모두 이벤트 기반 아키텍처의 최종적 일관성과 비슷한 느낌으로 관리하는 형태다.
근데 사실 95% 가 조회잖아? 3번 선택지도 썩 괜찮을 것이다.

### 데이터는 어떻게 저장되는가

가장 단순한 방안은 객체마다 개별 파일로 저장하는 것이지만, 68억개의 파일을 관리하는 건 굉장히 비효율적이다.

특히나 작은 데이터를 파일로 만들어 관리하는 건 낭비되는 공간의 수가 너무나 많아진다. (like OS 메모리 관리 - 페이징 기법 이전)
보통 디스크 블록의 크기는 4KB 정도가 되는데, 4KB 가 되지 않는 작은 데이터를 보관할 때도 온전히 4KB 를 다 사용해버린다.

그리고 inode 개수가 부족하다. 디스크를 최초 포맷하는 시점에 inode 개수를 결정하는데, 작은 파일 수가 많아지면 inode 가 모두 소진될 가능성이 높아진다.
다시 포맷해야함. 게다가 metadata 저장소 쪽을 캐싱하는 전략을 사용하려고 해도 inode 가 너무 많으면 효과가 떨어져버림.

그래서? 하나의 파일에 작은 데이터들을 덕지덕지 이어붙인다. 그리고 파일이 일정 크기 이상이 되면 읽기 전용으로 변경해버린다.
"어? 그럼 나중에 관리가..." 어차피 데이터저장소는 절대 데이터를 차갑게 관리함. 그리고 95% 가 조회요청이다.

파일별 데이터 주소를 빠르게 판별하고 접근하기 위해 파일 내부에 데이터는 일렬로 저장된다.
여러 코어가 병렬로 쓰기 연산을 시도하더라도 내용이 뒤섞여서는 안된다. 하나의 파일에 여러 코어가 달라 붙어 쓰기를 시도하면
처리량이 심각히 떨어지므로, 코어별로 전담 읽기-쓰기 파일을 가져서 관리한다.

### 객체 소재 확인

파일 안에 데이터가 들어가면, 어떻게 데이터를 빠르게 찾아낼 수 있는 걸까?
페이징 기법의 페이징 테이블과 동일하게 별도 정보를 저장해둔 테이블을 가진다.

- 데이터가 보관된 파일이 어떤 건지
- 그 파일 내 오프셋(시작 주소 판별)이 무엇인지
- 그 데이터의 크기가 얼마인지(끝 주소 판별)

이 테이블은 어떤 방식으로 관리하는게 좋을까? 95% 조회 연산을 진행하므로 조회 연산에 특화된 관계형 데이터베이스가 좋을 것이다.
모든 데이터 노드의 정보를 관리하는 배치 서비스에서 이 테이블을 가질까?
테이블에 저장될 데이터의 양이 어마무시할텐데, 꽤나 부담스럽다.
잘 생각해보면 알겠지만 데이터 노드끼리 서로 정보를 공유할 필요는 없다. 자기 노드의 파일, 데이터 정보만 기록하면 된다.
(어차피 서로 다른 주소를 다룬다는 뜻)
때문에 각 데이터 노드마다 RDB 를 활용하여 정보를 다룬다. 그리고 SQLite 가 파일 기반 RDB 로 아주 적격이다.

최종 데이터 저장 흐름은 아래와 같다.

<img width="773" alt="image" src="https://github.com/user-attachments/assets/6e7ec916-dab2-4002-8f61-1c582318219d">

<br>

## 데이터 내구성

데이터 내구성 계산식: `1 - {장애율}^{복제수}`

HDD 의 연간 장애율이 0.81% 라고 가정해보자. HDD 를 3대로 늘려서 복제하면 데이터 내구성은 아래와 같아진다.

```
1 - 0.0081^3 = ~0.999999
```

데이터 내구성을 더 디테일하게 평가하기 위해서는 문제가 발생했을 때 물리적/논리적으로 영향을 주는 범위, 서비스를 모두 고려하는 것이 중요하다.
(가령 PC 한대에는 CPU, 파워, HDD, 메모리 등이 있는데, 각각이 고장나면 PC 동작이 불가능하므로 이들이 모두 장애에 영향을 주는 개체들이다.)

규모를 키워서 생각해보면 DC 의 AZ(Availability Zone)을 예로 들 수 있다. 대게 DC 내부에서 여러 AZ 를 나누고,
데이터를 여러 AZ 에 복제하여 장애 여파를 최소화하려고 한다.
(규모를 작게 생각해보면 서버마다 파일을 분산시켜 저장하는 방식이 있을 것이다.)

온전한 데이터를 2개 더 복제하여 3중으로 다중화하면 약 99.9999% 의 내구성을 달성할 수 있다고 했다.
데이터를 다중화하면 복구할 때 계산도 빠르고, 다중화도 어렵지 않고 빠르고, 장애가 발생하더라도 발생하지 않은 지점의 데이터를 읽기 때문에 조회 성능도 뛰어나다.

다만 다중화는 추가 저장 용량 오버헤드를 발생시킨다. 예시만해도 200%.
객체 저장소는 데이터를 차갑게 관리하며, 대용량을 다루기 때문에 저장 용량을 아끼는 것이 가장 최우선 가치가 된다.

때문에 쓰기성능, 읽기성능이 다중화보다 떨어지지만 데이터 내구성과 저장소 효율성이 더 뛰어난 소거코드 기법이 권장된다.

소거코드는 데이터를 작은 단위로 분할하여 각기 다른 서버에 배치하면서, 일부가 소실되었을 때 복구하기 위한 패리티
(원본 데이터에 특정 알고리즘을 먹여서 만들어내는 추가 데이터)를 만들어 중복성을 확보한다.
소실되지 않은 데이터와 패리티를 조합/계산하여 소실된 부분을 복구하는데 사용한다.

<img width="679" alt="image" src="https://github.com/user-attachments/assets/23c6e786-d7c5-49e5-9c8b-5a6e00921f9c">

아래 예시를 통해 실제 소거코드를 어떻게 활용하는지 알아보자.

<img width="704" alt="image" src="https://github.com/user-attachments/assets/8bc4f333-afa8-4b9c-af74-17cdcdaf75e9">

1. 알고리즘을 통해 데이터를 8개의 원본 조각과 4개의 패리티 조각으로 만들었다. 총 12개의 조각은 모두 크기가 같다.
2. 각 조각들을 각 다중화 공간에 분산 배치한다.
3. 4개의 패리티 조각 덕분에, 최대 4개의 다중화 공간에서 동시에 장애가 발생해도 복원이 가능하다. 

예시에서는 데이터 하나를 조회할 때 8개의 다중화 공간을 뒤져서 조합해야한다. 조회 성능이 엄청 뒤쳐질 수 밖에 없다.
그러나 앞서 말했듯 객체 저장소는 공간 비용이 가장 큰 가치를 가진다.
3중 다중화로 300% 공간을 차지하는 것보다, 150% 공간을 차지하는 것이 이득이다.

<img width="693" alt="image" src="https://github.com/user-attachments/assets/06e91ad7-28a0-4e37-9582-7cd645edd2b2">

다중화와 소거코드의 장단점은 표와 같다.

AWS S3 는 어떤 방식을 채택하고 있을까? CF(CDN)을 이용한 지역 캐싱을 통해 조회 성능을 어느정도 확보하므로
소거코드와 같은 공간비용 최적화 방식을 채택하고 있지 않을까 추측해본다.

### 정확성

디스크 뿐만 아니라 메모리 위에 올라온 데이터가 망가지는 일도 있다. 메모리 속 데이터는
네트워크 패킷과 비슷한 방식으로, 프로세스 경계에 checksum 을 배치하여 문제를 해결한다.

원본 데이터의 checksum 을 알면, 메모리 위 데이터의 checksum 과 비교하여 데이터 정확성을 확인할 수 있다.

<img width="630" alt="image" src="https://github.com/user-attachments/assets/c2e8f72e-38a4-4439-ae40-70d4522aabe5">

작성중인 파일에는 데이터마다 체크섬을 배치하고, 용량이 차올라 읽기전용 파일로 전환할 땐 파일의 끝단에 자기맣게 체크섬을 배치한다.

소거코드는 데이터를 조각내어 분산배치한다. 소거코드와 체크섬을 함께 사용할 경우 아래와 같은 절차를 거친다.

1. 객체 데이터와 체크섬을 가져온다.
2. 수신된 데이터의 체크섬을 계산한다. 체크섬이 다르면 데이터를 복구한다.
3. 데이터 조각을 전부 수신할 때까지 1~2 절차를 반복한다.

<br>

## 메타데이터 데이터 모델

메타데이터 스키마는 아래와 같은 기능이 지원되어야한다.

1. 객체 이름으로 ID 찾기
2. 객체 이름에 기반하여 저장 또는 삭제
3. 같은 접두어를 갖는 버킷 내 모든 객체 목록 확인

(3번 같은 경우 디렉토리 탐색과 비슷한 논리적인 기능을 제공하기 위해 필요하다.)

사용자가 만든 bucket 정보가 담기는 bucket 테이블은 그 양이 그리 크지 않다.
그러나 사용자들의 읽기 요청을 모두 받아내어야 하므로 CPU, 네트워크 대역폭이 부족하다.
replication 을 통해 이 문제를 해소할 수 있다.

object 테이블에는 객체의 메타데이터가 보관된다. 객체의 수가 엄청 많기 때문에 부담된다.
때문에 sharding 을 통해 테이블을 분리하여 관리해준다.

sharding 기준은 무엇이 좋을까? bucket_id 는 특정 bucket 에 객체가 몰리는 경우 해당 shard 가 hot spot shard 가 되므로 좋지 않다.
object_id 는 균등하게 배치하기 좋겠으나, 객체 이름을 기반으로 다루는 경우 효율적인 조회가 어렵다.

객체 이름으로 ID 를 찾고, 이름으로 저장/삭제를 지우너하므로 결국 이름을 사용하는 것이 좋다.
책의 예시 설계안에서는 bucket_name 과 object_name 을 조합하여 sharding 기준으로 사용한다.
단, 이러면 `같은 접두어를 갖는 버킷 내 모든 객체 목록 확인` 이 조금 애매한데, 어떻게 해결할까?

### 버킷 내 객체 목록 확인

앞서 말했듯 객체 저장소는 객체를 계층구조로 보관하지 않는다. 수평경로를 다룬다.
특정 문자열을 기준으로 논리적인 계층을 모방할 뿐이다.

```
aws s3 list-buckets 
aws s3 ls s3://mybucket/abc/
```

요런 식으로 활용 가능!

### 단일 DB 를 이용한 객체 목록 확인

특정 사용자가 가진 모든 버킷 출력

```sql
SELECT * FROM bucket WHERE owner_id = {id}
```

같은 접두어를 갖는 버킷 내 모든 객체 출력

```sql
SELECT * FROM object
WHERE bucket_id = "123" AND object_name LIKE 'abc/%'
```

### 분산 DB 를 이용한 객체 목록 확인

sharding 이후 어떤 shard 에 데이터가 있는지 모르므로, 목록 출력이 어렵다.
가장 쉬운 해결책은 모든 shard 에 검색을 돌린 후 결과를 취합하는 것.
그러나 이 방식은 페이징 기능을 구현하기 복잡하다.

객체가 여러 shard 에 나눠져 있으므로, shard 마다 반환되는 객체의 수가 다르다.
shard 마다 추적해야하는 offset 이 달라질 수 있다.

때문에 bucket_id 를 기반으로 별도 샤딩 하는 목록 비정규화 테이블을 만들면 쉽게 해결할 수 있다.
객체 저장소는 규모와 내구성이 중요하고, 목록 출력 성능은 비교적 우선순위가 낮다.
때문에 이런 방식 채택을 고려해볼 수 있다.

<br>

## 객체 버전

객체 저장소가 차가운 관리가 가능하게 해주는 요인. 객체를 삭제한 경우 실제로 바로 삭제되는건 아니고,
삭제 마킹을 해두면 GC가 이를 회수한다.

<br>

## 큰 파일 업로드

큰 파일을 한꺼번에 업로드 하다가 중간에 네트워크 문제가 생기면 처음부터 다시해야하는 끔찍한 일이 생긴다.
이 문제를 해결하기 위해 객체를 잘게 쪼갠 다음 독립적으로 업로드하고,
업로드가 완료되면 조각을 모아 원본 객체를 복원하는 멀티파트 업로드를 선택한다.

<img width="532" alt="image" src="https://github.com/user-attachments/assets/9e0dffbd-df7a-459e-be50-a10c6456d9b8">

멀티파트 업로드 이후 객체조립이 끝나면 나머지 조각들은 쓸모가 없어진다.
이것들은 추후 GC 가 수거하여 처리할 수 있도록 해주면 좋다.

<br>

## 쓰레기 수집

- 지연된 삭제(Lazy object deletion): 삭제했다고 표시는 하지만 바로 지우지는 않는다. GC 를 기다린다.
- 고아 데이터: 반쯤 업로드 되거나 취소된 멀티파트 업로드 데이터가 존재할 수 있다.
- 체크섬 오염 데이터: 체크섬 검사에 실패하여 버려진 데이터.

GC 가 이것들을 모아 다 처리를 해야한다. 소거코드를 이용하여 데이터를 다루는 경우 객체 하나를 지울 때
GC 는 노드들을 전부 확인해야한다.

<img width="640" alt="image" src="https://github.com/user-attachments/assets/19c98c40-b9ca-4c79-b9d8-711d7b31ef3e">

GC 는 그림과 같이 파일을 복제한다. 복제 과정에서 삭제마킹이 되어있는 데이터들은 제외시킨다.
살아남은 작은 데이터들을 다시 하나의 파일로 모아서 공간을 확보한다.
